{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux/Reacher.x86_64',\n",
    "                       no_graphics = True)\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "state_size =  33\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "# size of each state\n",
    "states = env_info.vector_observations[0]\n",
    "state_size = states.shape[0]\n",
    "print(\"state_size = \", state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The displayed result shows that we take option 2 for the solution of the project, and there are 20 double-jointed arms to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DDPG Algorithm\n",
    "### 2.1 The DDPG\n",
    "\n",
    "The deep deterministic policy gradient (DDPG) method [1] is a model free reinforcement learning algorithm, and it is an extension of the deterministic policy gradient (DPG) method [2]. The difference between the two method is that, DPG considers the deterministic policies which considers that\n",
    "$$\n",
    "a = \\mu_\\theta(s)\n",
    "$$\n",
    "where a is the action, $\\mu$ is the policy, $\\theta$ is the parameters and s is the state.\n",
    "\n",
    "The DDPG method adopts the actor-critic approach with Deep Q Network[3] to form a model-free, off-policy reinforcement learning algorithm for the learning of optimal policies in high-dimensional and continuous action spaces problems, such as autonomous driving and robotics, etc. For the example problems, their actuators receives continuous command, such as throttle and joint torques. The DQN method can only handle discrete action space, for that reason, its application is limited.\n",
    "\n",
    "### 2.2 The actor-critic\n",
    "\n",
    "The DDPG uses stochastic policy for the agent, i.e.\n",
    "$$\n",
    "\\pi_{\\theta}(a|s)=\\mathbb{P}[a|s,\\theta]\n",
    "$$\n",
    "where $\\theta$ is the parameter vector, $\\pi$ is the policy.\n",
    "\n",
    "For this problem, the stochastic actor-critic method is applied. the actor is applied to find the optimal $\\theta^*$ in order to approach the optimal policy $\\pi^*$, that's to say, $ \\pi_{\\theta}(a|s)\\rightarrow\\pi^*_{\\theta}(a|s)$. For policy gradient method, the state-value function has to be estimated as well. In this approach, the critic is applied to adjust the parameter vector to approximate the sate-value function $Q^{\\pi}(s,a)$. Then, an approach similar to DQN method is applied for both actor-critic networks.\n",
    "\n",
    "For the process of training actor network, the training data are randomly picked from the **Experience Replay Buffer**. The predicted action $a_p\\{t\\}$ is generated via **Local** actor network fed by current state $s_t$. Then, an approximated action-value function $Q^\\omega(s_t,a_{p\\{t\\}})$. An unary minus of the approximated action-value function is directly used as the loss function for the update of the **Local** actor network. \n",
    "\n",
    "The update of critic network is even more complex. First of all, since we prefer to get the **Expected** action-value function, we use the state of the next time step $s_{t+1}$. An next time step action is guessed via the **Target** network of the actor. And The expected value function is generated via **Target** network of the critic, and the action value function is generated via **Local** network of critic. Then, the Bellman equation is calculated with the value function, and the mean-square-error loss function is applied for the update of **Local** network of the critic.\n",
    "\n",
    "Bear in mind that, for both actor and critic network, the Target network are slowly converged to the Local network through **soft update**.\n",
    "\n",
    "The **ReplayBuffer** class is a container which stores the past experiences. In the learn procedure, the past experiences are stochastically chosen and are fed into the two Q-networks. One Q-network is fixed as Q-target, it is denoted by$\\theta^-$. This Q-network is 'detached' in the training process, in order to achieve better stability. As a consequence, the change in weights can be expressed as \n",
    "$$\n",
    "\\Delta \\theta = \\alpha \\left[ (R + \\gamma \\max_{a} \\hat{Q}(s,a, \\theta^-) -\\hat{Q}(s,a,\\theta))\\nabla_{\\theta}\\hat{Q}(s,a,\\theta) \\right]\n",
    "$$\n",
    "DDPG is an off-policy algorithm, as a matter of fact, the exploration procedure can be conducted independently. This procedure is kind of policy gradient method. An stochastic actor is determined by the current policy, and noise generated by the **Uhlenbeck & Ornstein** method is added to it for searching the gradient direction, until it approaches the optimal policy. Thus the actor policy can be expressed as\n",
    "$$\n",
    "\\pi'(s_t)=\\pi(s_t|\\theta_t^\\pi)+\\mathcal{N}\n",
    "$$\n",
    "where $\\mathcal{N}$ is the noise for searching 'best' actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        #Specify a decay rate for the stocastic reach policy\n",
    "        self.epsilon = 1.;\n",
    "        self.epsilon_decay_rate = 0.999\n",
    "        self.epsilon_min = 0.8;\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR), weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        #Copy the weights from local to target networks\n",
    "        self.soft_update(self.critic_local, self.critic_target, 1)\n",
    "        self.soft_update(self.actor_local, self.actor_target, 1)\n",
    "        \n",
    "        # Noise for action exporation\n",
    "        self.noise = OUNoise((NUM_AGENTS, action_size), random_seed)\n",
    "\n",
    "        # experience replay buffer\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon*self.epsilon_decay_rate, self.epsilon_min)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, step):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        \n",
    "        #Put SARS into the replay buffer\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # perform learning process when enough experiences are stored\n",
    "        if len(self.memory) > BATCH_SIZE and (step % TRAIN_EVERY) == 0 :\n",
    "            for _ in range(NUM_TRAINS) :\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "            \n",
    "        self.actor_local.train()\n",
    "        #the noise is added with an decay\n",
    "        if add_noise:\n",
    "            action += self.epsilon * self.noise.sample()\n",
    "        \n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "                \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models        \n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next).detach()\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, shape, seed, mu=0., theta=0.15, sigma=0.08):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(shape)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * (np.random.rand(*x.shape)-0.5)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The model\n",
    "\n",
    "In this project, the Q-net is constructed by **three fully connected layers**. The architecture is the same as the network described in the paper [1]. But the units are reduced to reduce the computational time, since the problem is simpler.  In this case, the hidden layers are with 128 and 256 units respectively. For the input layer, the number of input node is the same as the number of states of the agent.  Finally, for the output layer, the number of output layer is the same as the action size of the agent. For the input layer and the out put layer, the output value is activated by the **Rectified Linear Unit** (ReLU) function. Since this is a continuous control problem, we have to use **tanh** function for the output of final layer. The network for the critic has the same structure as the actor network. however, the critic approximates the action-value function, its input should be states and action, consequently, the number of node for the input layer is the number of states plus the number of actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Process\n",
    "\n",
    "### 3.1 Hyper-parameters\n",
    "\n",
    "The hyper parameters for the learning process are generally utilized the parameters provided by  the paper [1]. However, some modifications are conducted for both convergence and stability. The WEIGHT_DECAY is set as 0. And I conduct one training process in very 25 time steps. In my  hyper-parameters tuning experience, TRAIN_EVERY influence the convergence significantly. At one training step, I set NUM_TRAINS as 5 to conduct 5 trains at a time. Other difference  is that I increase the minibatch size to 128 to allow more past experiences to be used for one training. Another improvement is that, I reduce the exploration noise a decay rate (say 0.999) to achieve better stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.     # L2 weight decay\n",
    "TRAIN_EVERY = 25        # how often to update the network\n",
    "NUM_AGENTS = num_agents\n",
    "NUM_TRAINS = 5\n",
    "\n",
    "agent = Agent(state_size, action_size, random_seed=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Learning Process\n",
    "\n",
    "The agent choose an action corresponding to  the current state via the Local network of the actor. And the action is applied to the environment, generates the reward of the action and the state, and the transmission of the next state. Than, they are stored in the Experience Replay Buffer for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Process Started! \n",
      "\n",
      "Episode 10\tAverage Score: 1.81\tScore: 2.48\n",
      "Episode 20\tAverage Score: 2.61\tScore: 3.78\n",
      "Episode 30\tAverage Score: 4.22\tScore: 7.95\n",
      "Episode 40\tAverage Score: 5.82\tScore: 11.75\n",
      "Episode 50\tAverage Score: 7.31\tScore: 14.69\n",
      "Episode 60\tAverage Score: 8.92\tScore: 17.72\n",
      "Episode 70\tAverage Score: 10.70\tScore: 23.24\n",
      "Episode 80\tAverage Score: 12.65\tScore: 25.71\n",
      "Episode 90\tAverage Score: 14.51\tScore: 29.67\n",
      "Episode 100\tAverage Score: 16.25\tScore: 32.52\n",
      "Episode 110\tAverage Score: 19.44\tScore: 34.34\n",
      "Episode 120\tAverage Score: 22.51\tScore: 35.45\n",
      "Episode 130\tAverage Score: 25.25\tScore: 35.36\n",
      "Episode 140\tAverage Score: 27.79\tScore: 35.27\n",
      "Episode 150\tAverage Score: 30.16\tScore: 36.05\n",
      "Training Process Ended! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes = 500, max_t = 1000, display_rate = 10):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    print(\"Training Process Started! \\n\")\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "\n",
    "        score = np.zeros(NUM_AGENTS)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            for i in range(20) :\n",
    "                agent.step(states[i], actions[i], rewards[i], next_states[i], dones[i], t)\n",
    "            \n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if np.any(dones):                                 \n",
    "                break\n",
    "        \n",
    "        agent.epsilon_decay()\n",
    "        score = score.mean()\n",
    "        scores.append(score)\n",
    "        scores_deque.append(score)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % display_rate == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor' + str(i_episode) + '.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic' + str(i_episode) + '.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "        if i_episode % display_rate == 0 and np.mean(scores_deque) >= 30:\n",
    "            break\n",
    "    \n",
    "    torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "    torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "    \n",
    "    print(\"Training Process Ended! \\n\")\n",
    "    return scores\n",
    "            \n",
    "            \n",
    "        \n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Experimental Result\n",
    "\n",
    "The Figure shows the learning procedure. With the prescribed structure and hyper parameters, the networks converges to the 'optimal policy' nicely with little oscillations. And the agent reaches the target average score 30 in 150 episodes, which means the network structure and the hyper parameters defined find a good balance point between exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8XGW5wPHfk8m+N3uzNW2SbmnpQijQFmTfKSi7qIgoekVFxQ3RK7jce1UWQQFFQOvCIiBQWVtKCxQKbbrvaZKmzb7v+8y89485mSZtti6TSTLP9/PJpzPvOTPnyWmSZ95djDEopZTyXX7eDkAppZR3aSJQSikfp4lAKaV8nCYCpZTycZoIlFLKx2kiUEopH6eJQCmlfJzHE4GI2ERki4i8Zj2fKiKfiEiBiDwvIoGejkEppdTgRqNGcCewp8/zXwMPGWOygAbgtlGIQSml1CDEkzOLRSQVWA78CvgucCVQAyQZY+wiciZwrzHm4qHeJy4uzmRkZHgsTqWUmog2bdpUa4yJH+48fw/H8TvgB0CE9TwWaDTG2K3npUDKcG+SkZFBXl6eZyJUSqkJSkQOjuQ8jzUNicgVQLUxZtNxvv52EckTkbyampqTHJ1SSqlenuwjWAIsE5Fi4DngPOBhIFpEemsiqUDZQC82xjxhjMk1xuTGxw9bs1FKKXWcPJYIjDF3G2NSjTEZwI3Au8aYm4E1wLXWabcAr3oqBqWUUsPzxjyCHwLfFZECXH0GT3khBqWUUhZPdxYDYIxZC6y1HhcBi0bjukoppYanM4uVUsrHaSJQSikfp4lAKaU8qKyxg7d2Vgx4zO5wjnI0A9NEoJRSHvS7Vfl87R+bqWvt6le+ancV83++iurmTi9FdpgmAqWU8hBjDB8W1AKwvqiu37H382to7bLzzp5qb4TWjyYCpZQ6Bk6nYaRrtBXVtlHe5PrE/2FB/0SwrbQRgHf2VJ3cAI+DJgKllBohYwy3/GUD33l+64jO760NzEyKYH1hrbu8s8fBnopmAmzChwW1tHfb2VrSyKUPf0CVF5qKNBEopdQIbThQzwf7a3l7VxVddsew53+wv5a0mBCuz02juK6dssYOAPZUNNPjMHx2UTpddifv59fy01d2sqeimTd2DNyx7EmaCJRSE47DaXh1axkO58ldZv+xtYWIQEePg03FDUOea3c4+biwjqVZ8SzJigMO1xC2lbiahb581jQigv257z+72FHWRKC/H6u90GegiUApNeF8WFDLnc9tZc3ek/dHdWdZE+/l13DHOVkE2IT39g+9KvK20iZauuwszYpjemI4ceGBfGQlgu2lTSREBJE6KYRzZiRQ0dTJgvRovrg4g08O1NHS2XPS4h4JTQRKqQmnsKYVcDXBnCx/fK+QiCB/bv/UNE6dMon382uHPP+D/TWIwOLMWESEMzPj+LCwji67g62ljcxLi0ZEuPKUyQTYhJ9dmcMFsxLpcZhh3/tk00SglJpwDtS2AbC3quWkvJ/d4WTN3mqWzU8mMjiAs6fHs6eimeqWgTt2jTG8urWcRRkxTApzbcv+mYUp1LR08c1ntlBU08b8tGgALspJYvNPL2R+WjQL06OJDg1g9SiPJNJEoJSacHoTwb7Kk5MI9lS00Nbt4PRpsQCcne3aI+WDQT65bzrYwIHaNq7LTXOXnTsjgR9eMpOVu11/5OelRruPRQQHAOBv8+PcGQms2Vd90vs3hqKJQCk1bq3dV83cn7191Cfz3kRwoLZtRKN7hrOxuB6A0zImATB7ciRx4YGszR+4n+CFvFJCA21cOiepX/nXPjWNL5w5hdBAG3NTowZ87QWzEmlo7+Hhd/JHPF/hRGkiUEqNWyt3V9HSZWfd/sOfzLvsDsoaO8hKCMfhNBRUtw77Pj0OJ7f+ZcOgQzc3FteTEh3C5KgQAPz8hEvmJPH2zkoqm/onofZuO69tL+fyuZMJC+q/0r+IcN+yHDbccwFRIQEDXuvinESuWZjKI+8W8L0XttNt9/x6RJoIlFLj1oYDrk/qHxUenrV7qK4dY+CSHNen8ZE0D72ypYw1+2p4dE3BUceMMWwsbmDR1Jh+5V89OxOHMfzp/cJ+5W/uqKSt29GvWagvESE8aPCtYPxtftx/3Sl854LpvLS5lFW7Pd9foIlAKTUu1bV2UVDdip/A+sI6dzNKb7PQuTMTCLT5DZsIHE7DY2sLCbAJu8qb2V3eTI/Dyc1PfsyDq/IprmuntrWLXKtZqFdaTCifXpDCM58ccjdNdfY4+P27+8mMD3M3Ix0PEeHOC7J55Y4lXH7K5ON+n5HyWCIQkWAR2SAi20Rkl4jcZ5X/VUQOiMhW62u+p2JQSk0M3XYnFU0d/co2WhO6rpyXTFljByX1ruO9iSArIZzMhHD2DpMIXt9RwYHaNn5+1RwCbX68sKmE5R8V82FBHY+s3s8DK/cBsCgj5qjX3nFuFj0OJw+/sx+n0/DI6v0U17Xz86vmICIn/H33jizyNE9uVdkFnGeMaRWRAGCdiLxpHfu+MeZFD15bKTWB/GFNAU+vO8DGey4gJNAGuJqFgvz9+OrZmby6tZyPCmtJj03nQG0bsWGBRIUEWGv81B31fj0OJz94cTsN7d3sqWgmOyGcG3LTWLe/lpe3lGF3GM7KjqOmpYvXtlcQHRpAZnz4Ue8zNS6Mmxal889PDrGzvJldZU1ce2qqeybxeOGxGoFx6e2lCbC+Rm88lFLqpGjvtvPDF7cf9Ym8qaOHSx/+gHf3Hm7D9tQol7d2VtDaZWe7tWInuDpwF6RHM2tyBPERQe5+ggO1bUyNCwNgRlIElc2dNLZ393u/9YV1vLyljLKGDiaFBvLjy2bh5ydcm5tKY3sPXXYH9y3L4eEbFxDo78dpGTH4+Q38Cf+XV8/ht9eewqG6NqJDA7jnslkeuQee5NE+AhGxichWoBpYZYz5xDr0KxHZLiIPiUiQJ2NQSp2YvOIGns8r4dkNJf3K/76+mD0Vzazd5xpC6XQaznvgPR5Zvf+kXr+kvp38Ktdnyk2HXM1BLZ097CpvYlFGDCLC4sxYPrL6CQ7UtpHRJxEARzUPvbWrktBAG//55lLe+vbZnDszAXDND8hJjuRb52UzLT6cGUkRvPS1xdy7LGfQ+ESE63LTeO8H5/LmnWe7J5CNJx5NBMYYhzFmPpAKLBKROcDdwEzgNCAG+OFArxWR20UkT0TyamqGXtNDKeU5vcMv395Z6S5r67Lz1LoDwOE/siUN7RyobeMPawooqW8f8L1W76lia0njgMcG0zvLNiokwL3Q26aDDTgNLJrqmuC1JCuO2tYufvP2Pqpbutw1gtmTI4H+S004nIaVuyo5d2YCwQG2ftey+Qmvf+ssvnl+trtsbmoUKdEhw8YZGRxAfMT4/Fw7KqOGjDGNwBrgEmNMhdVs1AX8BVg0yGueMMbkGmNy4+PjRyNMpdQA9luJYF9Vi7sj9tkNh2ho7yEnOZL8qhaMMewud/2x7bY7+fVbe/u9hzGGP7y7n9uW5/GL13YPe83tpY3uuQGr91YzLT6Mi2YnsulQA8YY3t5VSUiAjYVTXJ2pV81P5tMLUnh8rWsoZ28iSIgIIi480B0bQF5xPbWt3UdN9vJlnhw1FC8i0dbjEOBCYK+ITLbKBLga2OmpGJRSJ66guoX0mFAA3t5VSXNnD0+8X8TizFiuO9XVpl7T0sXuimZsfsJXPzWN17ZXsOlgvfs97l+5j/tX5hMR5E9+ZcugfQk7Spu4/o/rWfaHD/ncU5/wxPuFfFJUz/kzE8jNmERjew+7ypv5z7YKLj9lMqGBrvEuQf42Hrx+Hvcty2FafBgL011DN0WEWZMj2dUnEby1q5JAfz/OmZHgqVs27nhy1NBkYLmI2HAlnH8ZY14TkXdFJB4QYCvwNQ/GoJQ6AcYY8qtauWzuZKLKmnhjRwUfFtRS39bNXRfNcC/fsLeyhd3lzWTGh3Hn+dms2FrO91/czmvfXEpecQOPrinkxtPSyEmO5Kev7qKssYPUSaH9rtXZ4+Drz2yis8fJTy6fxfrCOv7nDVfN4vxZicSFu9ref/n6blq77Fx/xIQtEeGWxRncsjijX3lOchRPrSui2+4kwCa8vbOSs7Pjh5zU5Ws8dieMMduBBQOUn+epayqlTq7a1m6aOnrITggndVIIv33bNab+19fM5dQpk6hr7QIgv6qF3RXNnD41htBAfx64fh43P/kJP3ppBxuL68lKCOfeZTnsLGsCXLN9j0wET394gJL6Dv755dNZkhXH58+cwjee2cKeimZOnTIJmwjRoQF8XFRPRmzoiCdszU6OpMdh2F/dQo/DUN7UyV0XzTiJd2n805nFSvkYYww9jpGtX7O/2tURnJ0YzmVzJxPo78d/nZPJDaelAxAbHkRceBAfF9VR0dTJ7GRX5+zizDjuOCeLFdvKqW7p4oHr5hEcYCM7ceBRPNXNnTz6bgEXzEp0j8EP8rfx5y/k8u5d5xBg88PPT9xNPtflpo14wlaOFdPu8mbe2lmJv59wwazEEb3WV2jdSKkJrrc9vvcP50uby/jFa7tZ98Nz3csfD6Z3xFB2QgRJUcFs+skFR71mZlKEewjp7MmHV9T89gXZlDS0Mz8tmnnWDNmokACSo4KPWvbhoXfy6XY4uefyo8fgB/of/ry6ODOWdftruWZh6oi+d4CM2DBCAmzsKm9m7b5qzsyMJSp06O/b12iNQKkJ7vH3CvnUb9e6V7H8z7Zymjp62HRw6D13wZUIIoL8SYx0DYscKHFMT4zAbq2dP2tyhLvc3+bHwzcu4NYlU/udPyMpol8iOFTXzgt5pXx2Ubp7tM9gblmcwZrvn0NSVPCwsfey+QmzJkfwxo4KiuvauURHCx1FE4FSE1h9WzePvlvAofp2PiyopaPbwfoi1wzc3pU7h7K/qpWsxPAhm2FmWpO2kiKDiQ0ffhz9jKRICmta3YnpD2v24+cnfP3crGFfG2DzG9GY/iPNTo6kuqULEbhotiaCI2nTkFIT2BPvF9He4yAkwMZr2yswGLrtTkICbCNLBNWtnDtj6Hk8061E0Ns/MJyZSa4aRFFtK6EB/ry0uYzPnzGFxMiRf8o/VjnJriar0zJixu2kL0/SRKDUBFXb2sXyj4pZNi8Zfz8/Vu52dZSGBNi44bQ0nvnkEJ09jqNm1/ZqaOumtrWL7MSjF1vrKzshnEB/v35bLw6ld9mHXWXNvGl13v7XOZnH9s0do7kprkSgk8gGpk1DSk1Qf19/kC67g2+dn80V8ybT0mnnxc2lLMmK46zsOLodTvdyD3aHk82HGnh1a5m7cznf2vg9OyFi0GsAhAX589o3l/KVs6cOeV6vzPhw/P2En63YxTt7qvjBJTM9WhsA18ihp27J5ebTp3j0OuOV1giUmqC2ljQyMymSzPhw0mNCiQ4NoLG9h3NnxpM7JQYR2HigntZOO3e9sI2mjh4AEiKCOTMzlu2lrjH/c1IG3lu3r+mJQyeLvgL9/ciMD2dfVQv3Lcs5agKYJ4gI5+uQ0UFpjUCpCWpfZYu7IzfA5ufeuvGcGQlEhQYwIzGCf28p445nNpM6KYT7r5sH4F4aYltpIynRIR5pU//5VTn89dbTRiUJqOFpjUCpCaipvYfK5k53Ry7Ady+azrkzE9yjbhZNjeFv6w+SERvK8i8tIi48iD+9V+geVrqttJF5acPXBo7H6dNiPfK+6vhojUCpCWif1b4/o08iSIgI5uKcw52lV5ySzLzUKHcSADh1yiQ2HWygtrWLkvqOEXcAq/FNE4FSE5A7EQzRdr9oagyvfmMpU2IPT+I6dcokmjvtvLy5DMA9I1hNbJoIlJqA9lU2ExHsz+RjmIELrkQA8NePivGTw8Mu1cSmiUCpMaimpYvvPL+V17aXH9fr91W2MCMxYsQLs/WaGhdGTFggZY0dZCWEE6ZLNfsETQRKjTHrC+u47JEPeHlLGf/7xl4czoE3cfn35lJeyDu8j7Dd4aSzx4ExxpUIkkY+pLOXyOEVPrV/wHdoIlBqDOm2O7n9b3lEBPtz14XTKWvs4N291YBrAbiq5k7AtaLob97ax4Or8t2v/ckrO7nooffZX91Kc6f9uBIBHG4e0v4B36GJQKkxZEdZEy1ddn5w8Qz+65xMkiKD+dv6YraXNnL5Ix9w53NbACiqbaOyuZOKpk7KGzswxrBmXzWH6tu5/W95wNAdxUM5f1YC8RFBLLX2BVATn8caAEUkGHgfCLKu86Ix5mciMhV4DogFNgGfN8Z0eyoOpcaT3oXgTsuIwd/mx2dPT+fBVfnsqWimy+5kw4F6alu7+Kig1v2aTQcbmJsSRVVzFzOTItybvhxvjWB6YgQb77ngxL8ZNW54skbQBZxnjJkHzAcuEZEzgF8DDxljsoAG4DYPxqDUuLLhQB1ZCeHu5ZxvXJRGgE3o6Hbw0A3zcBp4Z3cV6wpqSY4KJiTAxqaDDe4E8rsb53P61BimxIYSHRrozW9FjSOe3LPYAK3W0wDrywDnAZ+1ypcD9wKPeyoOpcYLh9OQV9zAlfOT3WUJEcH8/qaFJEYGMT8tmgdW5vPGzkq2HmrgkjlJlNR3sOlgA82dPcSEBTIjMYLlX1pES6fdi9+JGm88OjZMRGy4mn+ygEeBQqDRGNP7U1oKpHgyBqXGiz0VzbR02Tl9aky/8r47al2Sk8ST6w4AsCQrjoLqVh5bW0hVcyeLMmIQEYIDbIMuLa3UQDzaWWyMcRhj5gOpwCJg5khfKyK3i0ieiOTV1NR4LEalxopP+vQPDObiPklhcWYcp06ZhMNpqG7pYtHUwV+n1FBGZbaIMaZRRNYAZwLRIuJv1QpSgbJBXvME8ARAbm7uwAOplRrn7A4nX1qeR3pMCIXVbaTFhJA8xFaMC9MnERceSGxYEPERQSxIn4QIGAOnT9NEoI6PJ0cNxQM9VhIIAS7E1VG8BrgW18ihW4BXPRWDUmNdaUMH7+cfrvFeszB1yPNtfsJDN8wnyN/V9BMVEsD0hAjKmzqYmTSyrSKVOpInawSTgeVWP4Ef8C9jzGsisht4TkR+CWwBnvJgDEqNacV1bQA8cN088qtbuGre8F1mZ2X330P4jvOyaGzvxuZ3bMtJKNXLk6OGtgMLBigvwtVfoJTPO1jXDsBZ2XFcc+rQtYHBLJuXPPxJSg1BZxYr5UXFdW2EBto8sguYUiOliUApLzpY186U2LBjXiVUqZNJE4FSJ6CpvYen1h1gT0UzrjmUx6a4ro2M2FAPRKbUyOli40qdgBXbyvjFa7sByEmO7Lft43AcTkNJfTsXzU4a/mSlPEhrBEqdgJKGDoL8/bhvWQ57K1t4ZPX+Ic/vtju5+987KK5to7yxgx6H0RqB8jqtESh1Akob2kmZFMItizPIr2rhmU8OceuSqUyNCxvw/K0ljTy74RBB/n5cMCsRoN+ewUp5g9YIlDoBZQ0dpE5yfaK/84JsAv39+L8397DpYAOr91ThPGJ3sR1lTQC8vauSA7WuNRkz4rRGoLxLE4FSJ6C0oYPUSa4lIRIigvnyWdN4e1cV1zz+Ebctz2Ndn30DAHaUNgJQ0dTJim3lBPn7kRhxbBvMK3WyaSJQ6ji1d9upa+smpc/aQF8/J5NfXzOXP37uVMDVFNTX9rImFk2NIcAmbCxuYEpsKH46I1h5mfYRKHWcyho6ANw1AoDgABs3nJYOwLS4MHdTEEBLZw9FNW1cPT+FkAAb7+XXaP+AGhO0RqDUcSpt7E0EA7fxz02NYmefRLCrvNld3rvHgI4YUmOBJgKljlPpADWCvuamRFHR1ElNSxcAO0qb3OUXzU4kMtifBemTRidYpYagTUNKHafShnYCbX7EDzKBbG5KFAA7y5o4d2YC28uaSI4Kdk842/zTC/G36Wcx5X36U6jUcSpt6CBlUsignb05KVGIwHarJrCjtJG5qVHu45oE1FihP4lKHaeyPkNHBxIe5O/uMG5o66a4rp1TUqNHMUKlRkYTgVLHoK3L7h4SWtrQ0W/o6EBOSY1me2kj33thGzY/4VPT44c8Xylv0ESg1DF4YGU+Vz/6Iev211Lb2jVkjQBgTkoU1S1drN5bzb1XzmZOStSQ5yvlDZoIlBrCzrIm/vLhAQA6uh28uKkEgG8/vxUYfOhorwXprqagW86cwufPzPBcoEqdAE9uXp8G/A1IBAzwhDHmYRG5F/gK0Ltj94+NMW94Kg6lTsTjawt5fUcFKdEhNHX00Nxp5ytnTeXPH7iSQ8owNYIFadG8cscS9wgipcYiTw4ftQN3GWM2i0gEsElEVlnHHjLG3O/Bayt1wowxfFxUB8C9K3YxKSyQrIRwfnzZLA7WtbNydxVpw9QIRIT5adpBrMY2jzUNGWMqjDGbrcctwB4gxVPXU+pkK6hupa6tmxty0yhv6mRXeTM3n56OiPCba0/hj59bSFKULhinxr9R6SMQkQxgAfCJVfQNEdkuIk+LyIBTK0XkdhHJE5G8mpqagU5RyqN6awN3nJvFzaenExHkz2cWpAIQHRrIJXMmezM8pU4ajycCEQkHXgK+bYxpBh4HMoH5QAXwwECvM8Y8YYzJNcbkxsfrkDs1+j4uqmdyVDBpMSH8/Ko5rP3+OUSFBng7LKVOOo8mAhEJwJUE/mmM+TeAMabKGOMwxjiBPwOLPBmDUsejt3/gjGmxiAg2PyF2hHsRKzXeeCwRiIgATwF7jDEP9invW5/+NLDTUzEodbx6+wfOmBbj7VCU8jhPjhpaAnwe2CEiW62yHwM3ich8XENKi4GvejAGpY7LxwfqAThjWqyXI1HK8zyWCIwx64CBVuPSOQNqTDPG8OqWMtJiQkiP0f0C1MSnM4uVOsL6ojryDjbwlbOm4WrhVGpi00Sg1BF+v7qAhIggrs9N83YoSo0K3ZhGKeB/39jDim3lLM2KY31RHT+9YjbBATZvh6XUqNAagfJJPQ4n3Xan+/k7e6po73bw7y1lxEcE8dlF6V6MTqnRpTUC5ZO+/s/N+PsJj3/uVNq77RTVtvGt87L5/JlTcDoNIYFaG1C+QxOB8klbDjXQ0e3A7nCyt7IFY2B2cqR7P2GlfIkmAuVzGtu7qW3tBmBvZQu7ypsByEmO9GZYSnmNJgLlcwpr2tyPNx1sYG9lC1EhAcNuO6nURKWJQPmcwppWAAL9/cg72MChujZykiN1zoDyWTpqSPmcwppWAm1+nD8zgY0H6tlb2aLNQsqnaSJQPqewuo2MuFAWTY2hsrmTLruT2ZoIlA8bcSIQkaUicqv1OF5EpnouLKU8p6imlcz4cHKnHF5ZNCdZ9xRWvmtEiUBEfgb8ELjbKgoA/uGpoJQ6UXsrm2lq7zmqvMfh5FB9O5nx4cyaHEFooI0gfz+mxYV5IUqlxoaR1gg+DSwD2gCMMeVAhKeCUupEdPY4uOaxj/juv7YedexgXTt2pyEzIQx/mx+LM2NZkB6Nv01bSZXvGumooW5jjBERAyAi+vFJjVnrC+to63awem8120oamZcW7T7WO2JoWlw4AA/dMB+n8UqYSo0ZI/0Y9C8R+RMQLSJfAd7Btc2kUmPOu3urCQmwER0awO/eye93zJ0I4l2fZSKCA4gK0X2IlW8bUY3AGHO/iFwINAMzgP82xqzyaGRKHQdjDO/urWZpdhzz06L57dv72FrSyHyrVlBY3UZiZBARwfrHX6lew9YIRMQmImuMMauMMd83xnxvJElARNJEZI2I7BaRXSJyp1UeIyKrRGS/9e+kk/GNKAWQX9VKWWMH581M4JbFGcSEBXLff3ZhdzjpsjvYfKiBzPhwb4ep1JgybCIwxjgAp4gc6/g6O3CXMWY2cAZwh4jMBn4ErDbGZAOrredKnRTv7q0G4NwZCYQH+XPvshy2HGrkj+8V8j+v7+FAbRu3LM7wbpBKjTEj7SxuxbUJ/SqskUMAxphvDfYCY0wFUGE9bhGRPUAKcBVwjnXacmAtrqGpSh23P71XSEN7D+/sqSInOZKkqGAAls1LZtXuKh5clY/TwG1Lp3JxTpKXo1VqbBlpIvi39XVcRCQDWAB8AiRaSQKgEkg83vdVCqCmpYv/fXOv+/n3L57R7/gvrsphU3E9ydEh/OjSmaMdnlJj3kg7i5eLSCAw3SraZ4w5erbOAEQkHHgJ+LYxprnvwl59h6QO8LrbgdsB0tN1tyg1uP1VLQD8/bZFnJISTWRI/x/r6NBAVn73UwT7++l8AaUGMNKZxecA+4FHgceAfBE5ewSvC8CVBP5pjOmtUVSJyGTr+GSgeqDXGmOeMMbkGmNy4+PjRxKm8lH7q11DQmckRhAVGjDgKqLhQf6aBJQaxEh/Mx4ALjLGfMoYczZwMfDQUC8Q12/jU8AeY8yDfQ6tAG6xHt8CvHpsISvVX35VC5HB/sRH6O5iSh2PkfYRBBhj9vU+McbkW5/2h7IE+DyuTubeuf4/Bv4P1wS124CDwPXHGLNS/eyvbmV6YoTuJ6DUcRppIsgTkSc5vNDczUDeUC8wxqwDBvvNPH+E11U+oq3LTljQse+TZIxhf1ULl8zRkUBKHa+RNg39F7Ab+Jb1tdsqU+qEvbu3ioW/WEV1c+eIzrc7nPz3qzspqmmlrq2bhvYeshN0DUSljtdIP4L5Aw/3tvWLiA3QBll1UmwsbqDL7mRXRTMJkcHucqfT8NqOCi6bk9Svo3dfVQt/W3+Q9m4Hn1mYAkB2os4WVup4jbRGsBrou7N3CK6F55Q6Yb3DP4v6bCoP8PGBOr717Bbe2VPVr/xAreu8N3ZUsK2kCYDpiVojUOp4jTQRBBtjWnufWI9DPROS8jW9wz97VwbtVVLfDsD20qZ+5QeshNHe7eDJD4qICPYnQUcMKXXcRpoI2kRkYe8TEckFOjwTkvIlHd0ODll/8Aur+yeCsgbXj9iOsv6JoKi2jclRwUyLC6OurVtHDCl1gkbaR/Bt4AURKbeeTwZu8ExIypcU1rRiDESHBlBU279pqLRPIjDGuP/YF9W2MS0+jMWZcfz27X1kJ2j/gFInYsgagYicJiJJxpiNwEzgeaAHeAs4MArxqQku3+ofuHBWIjUtXTR1HF65pLTRlQga23vcScEYw4GaVqbFhXPNwlSCA/zKZTa2AAAYZ0lEQVTcew0opY7PcE1DfwK6rcdn4poQ9ijQADzhwbiUj8ivaiXAJpw3MwGAoj79BGUNHe5N5Xubh+raumnutDM1LoykqGA++tH5XJebNvqBKzWBDJcIbMaYeuvxDcATxpiXjDE/BbI8G5ryBfurWpgWF86MJNeon96RQ3aHk8rmTs6flYC/n7gTQe+IoanWVpMxYYHY/LR/QKkTMWwiEJHefoTzgXf7HDv2aaBKHSG/uoXsxHDSYkLx9xP3yKGqli4cTsNUK0nstBJBb40hM077BZQ6WYZLBM8C74nIq7hGCX0AICJZQNNQL1RqOO3ddkrqO8hOiCDA5seU2FB3IugdMZQyKYS5KVHuDuOi2jYCbELKpJCh3lopdQyGTATGmF8BdwF/BZYaY3r3DvADvunZ0NREV2ANF51uzQrOjA93Nw2VNbqGlKZOCmFOSpS7w/hATRtTYsO0OUipk2jY5h1jzMcDlOV7JhzlS/ZVukYMZVuzgqfFh7NmXzV2h/NwjSA6hIXpkwD4w7sFFNa06ubzSp1kulOH8pqPi+qZFBrgHhmUGR9Gj8NQXNdGWWMHceGBBAfYmJ0cyR3nZvJ8XgmFNW3ujmKl1MmhiUB5hTGGdQU1LM6Kw89q5lmcFYcIrNhWQWlDBynRh/sBvnfRDD5/xhQArREodZLpyB/lFQXVrVQ1d3FWVpy7LCU6hKVZcbyYV0Kgvx+zkyPdx0SE+5blsCQrjk9N161LlTqZtEagvOKD/bUALM2O61d+w2lplDd1UlzX3q9GAODnJ1wyJ4mQQNuoxamUL/BYIhCRp0WkWkR29im7V0TKRGSr9XWZp66vxrZ1BbVkxIaSOqn/IrYXzk4kOtS1C+qRiUAp5RmerBH8FbhkgPKHjDHzra83PHh9NUZ12518XFR3VG0AIMjfxtXzXZvNpEzSlc6VGg0eSwTGmPeB+mFPVBNeRVMH/9lWTu80lC2HGmjvdrA0a+C2/luXZHDGtBgWpOtickqNBm90Fn9DRL4A5AF3GWMavBCDGkUPrMznxU2l7K5o5tbFGfz01Z2EBto4MzN2wPOnxIbx3O1njnKUSvmu0e4sfhzIBOYDFcADg50oIreLSJ6I5NXU1IxWfMoDPjlQR2igjcfXFnLhQ+9T2tDBk7fkEhUS4O3QlFKMciIwxlQZYxzGGCfwZ2DREOc+YYzJNcbkxsfrcMHxpLql0z1ruLyxg5L6Dr574XRuWpSOCPz9tkUszjy6f0Ap5R2j2jQkIpONMRXW008DO4c6X40/doeTLz69kZKGdjb8+AI2Fru6ic6YFsuXz5rGL67Kwd+mo5aVGks8lghE5FngHCBOREqBnwHniMh8wADFwFc9dX3lHX/5sJjdFc0AvLGjgk2HGogI9mfWZNfkME0CSo09HksExpibBih+ylPXU95XUt/Og6vyOX9mAkW1bTy/sYS6ti5Oy4jR1UKVGsP045k6ZgXVLRysazuq/Pfv7kcEfn71HK7PTWNDcT2FNW0smhrjhSiVUiOliUAdkx6Hk889uYEfvrT9qGObDjawJCuOlOgQrjk1xV0L0ESg1NimiUAdk3d2V1HZ3Mn20iYcTuMub+uyU1TbRo61UFxCRDAXzEogLNDGnOQob4WrlBoBXX1UHZO/rT8IQHu3g4LqVvem83sqmjGGfn/0f/XpuVQ2dRLor583lBrL9DdUjVhBdQvri+q49tRUALaVNLqP7Sp3jRSak3I4EcSFB/V7rpQamzQRqBH7+/qDBNr8+NGlM4kI8mdb6eFEsLOsidiwQBIjg7wYoVLqeGgiUCO2cncVF8xOIC48iLmpUf0TQXkzOSlRiOgwUaXGG00EakRqW7uoaOp0byQ/Ly2avRUtdPY46LI72F/Vwpw+O4oppcYP7SxWI9LbB5BjdQbPS43C7jTsqWjG388Pu9O4jymlxhdNBGpEdpU3Abj3EZ6X5torYFtJI0EBrq0j56RojUCp8UgTgRqUMcbd5r+rrJm0mBD30tFJkcHERwTx7IYS/G1CRJA/abqjmFLjkvYRKLfi2jbau+0ANLR1c9kj63hsbQHgqhH0nSMgInxmYQp1bV2UN3ZwxbzJ+Ol6QkqNS1ojmOCcTsN7+TWcPT1+yIXf6lq7uOh37zMtLoy/3HoaP3hxO3sqmqls6uD63DSK69rd8wd63X3pLO6+dJanvwWllIdpjWCCe3FTKbf+dSNv7KgY8rzXd1TQbXdyoLaN8+5/jw/21/KZhSk0tPfw4Kp8AHJ0cphSE5Imggmsy+7g4dX7AVi3v3bIc1/dWs6MxAhe+NqZRAT788XFGdx/7TzSY0J5dsMhAPc6QkqpiUUTwQT27CeHKGvsICU6hA8LB08EJfXtbDrYwFULkjklNZr1d5/Pvcty8PMTblyUhjGQEBFEQkTwKEavlBotmggmqI5uB39YU8jpU2O4/explDZ0cKiufcBzV2wrB2DZvGSAfn0J156air+faG1AqQnMY4lARJ4WkWoR2dmnLEZEVonIfuvfSZ66vq97d281ta1d3Hl+NkuyYgEGrBUYY3h1axmnZUwidYDhnwkRwdx/3Ty+dX62x2NWSnmHJ2sEfwUuOaLsR8BqY0w2sNp6rjzg46I6wgJtLJoaQ2Z8OAkRQawrODoR7ChrIr+qlavmpwz6XlcvSGFBuuZspSYqjyUCY8z7QP0RxVcBy63Hy4GrPXV9X/dxUR2nTY3B3+aHiLAkK471hXXUtXbx4qZSWjp7AHh2wyFCAmwsm5/s5YiVUt4y2n0EicaY3nGMlUDiKF/fJ9S2drG/upXTp8a6yxZnxlLf1s3p/7Oa772wjR+/vJOWzh5e3VrOlfMmExkc4MWIlVLe5LUJZcYYIyJmsOMicjtwO0B6evqoxTURfFLkqoidMe3wXsHnzUzglNQo5qVGY/MT/vpRMR3ddtq7HXz29CneClUpNQaMdiKoEpHJxpgKEZkMVA92ojHmCeAJgNzc3EEThjpab/9A393BYsODWPGNpYBrA/q8g/W8s6eaWZMjmZeqE8WU8mWj3TS0ArjFenwL8OooX3/CKWvs4Hfv5PfbSP7jojpyM2IIsA383xtg8+P+6+YREmDj1sUZupmMUj7OYzUCEXkWOAeIE5FS4GfA/wH/EpHbgIPA9Z66vq+45+UdrN1Xw+LMOBZNjXH3D3xmYeqQr5uZFMnmn15IcIBOJVHK13ksERhjbhrk0PmeuqavWbOvmrX7agBYt7+GRVNj3EtJ9O0fGExIoM2j8Smlxgf9ODhO9Tic/Or1PUyJDeWU1CjetxLAGzsqSIwMYl5qtJcjVEqNF5oIxqGC6la++vdNFFS3cs9lszh3RgLbSxspb+xgbX4Nl87RvQGUUiOniWCceX7jIS7+3ftsOFDP3ZfO5MLZiZyVHYfTwC9f30233cllcyd7O0yl1DiiG9OMI5sPNfCTV3ayODOWh26YT1x4EODaPzgiyJ83dlSSEBFE7hRdDkIpNXJaIxgnalu7+Po/NjM5KoQ/3LTQnQTANRz0jEzXLOJL5iRps5BS6phoIhgnHlqVT317N49/biFRoUcvB3H29HgALtdmIaXUMdKmoXGgrcvOK1vKWDYvmZzkgWcB35CbRuqkEBZNHX7YqFJK9aWJYBxYsa2ctm4HNy0afM2lQH8/zp2RMIpRKaUmCm0aGoOMMWwraWR9YR3GGJ7dcIgZiREsTNe5AUqpk09rBGPMhgP1/PerO9lb2QLAgvRotpc2cd+yHF0TSCnlEZoIxphfvbGHhvZufnn1HHocTh5clU9ooI2rFwy+g5hSSp0ITQRjSHVzJ9tKGvn+xTP43BmuPQKump9CY3s3USG6cYxSyjM0EYwhq/e6tme4YNbhjdtiwgKJCQv0VkhKKR+gncVjyDu7q0iLCWF6Yri3Q1FK+RBNBGNEe7eddQW1XDArUTuFlVKjShPBGLFufy1ddicX9mkWUkqp0aCJYIxYubuKiGB/TtOZwUqpUeaVzmIRKQZaAAdgN8bkeiOOsaKj28FbOyu5OCdp0H2GlVLKU7w5auhcY0ytF68/ZqzcXUlrl51rTx16n2GllPIE/fg5Bry4qZSU6BBO12YhpZQXeCsRGGCliGwSkdu9FMOYUNHUwbqCWq5ZmKL7CCilvMJbTUNLjTFlIpIArBKRvcaY9/ueYCWI2wHS0wdfdXO8e3lLGcbAZxZqs5BSyju8UiMwxpRZ/1YDLwOLBjjnCWNMrjEmNz4+frRDHBXGGF7IK+W0jElkxIV5OxyllI8a9UQgImEiEtH7GLgI2DnacYwFHxfVc6C2jRtPm7g1HqXU2OeNpqFE4GVr9qw/8Iwx5i0vxOFRyz8qZl5aNPPTXHsIPPlBEa/vqKCsoYNL5yRx77Icntt4iIhgfy7T7SWVUl406onAGFMEzBvt646mQ3Xt/GzFLqbGhbHyO2ezu7yZX76+h9mTI5mRFMHy9QdJiAzmzZ2V3HhaGiGBNm+HrJTyYbr6qAe8tLkUgAO1bTzzySHe3FlBTFggz3/1DMIC/bn1rxv57dv7ALRZSCnldZoITjKn0/DvLaUszYrD4TT8zxt76LI7uffK2UQEu/YUeOiG+VzxyAckR4cwOznSyxErpXydJoKTbGNxPSX1HXz3wulkJ0Rwxe/XMSU2lM+ePsV9TkxYIG9/52yMF+NUSqlemghOgNNpeGxtAQE2P+akRJEQEcQzGw4RFmjj4pwkQgP9+f1NC8hKCCfQv/8Ard7agVJKeZsmghOQd7CB+1fmH1V+7amphAa6bu2V85JHOyyllDommgiGUNrQzv+9uZdvX5BNVkLEUcdXbCsjOMCPVd/5FIfq22lo76aj28F5MxO8EK1SSh0fTQSDMMZwz8s7eS+/hq0ljbz89SXERwS5j/c4nLyxo5LzZyWSFhNKWkyoF6NVSqnjp6uPDmLFtnLey6/hs6enU9vaxZf/lkdbl919/MOCWurbulmmTT9KqXFOEwHQZXewtaQRY1zjeCqbOvnFa7uZlxbNL66awyM3LmBHaSPXPP4Rh+raAVeiiAj255wZE3MdJKWU7/D5pqHKpk6+9o9NbC1pZGlWHNecmsIvX9tDe7eD//30XGx+wkU5SSz/0iK+8cwWLv/9B8yeHMm20kauPCWZIH+dFayUGt98ukZQUN3CFb9fR35VC185aypbSxr5zvPbiAkLZMU3lvSb7HVWdjz/+cZSzs521QBOSY3mi0syvBS5UkqdPD5dI3h8bRGdPQ5euWMJ0xMjuG3pNFbtqeKahSnu4Z99pceG8ujNC70QqVJKeY7PJoLWLjtv7Kjg6gXJTE90DQ1Nigrm82dMGeaVSik1sfhMIuhxOOmyOwmwCUH+Nl7fXk5Hj4NrT03zdmhKKeVVPpEIHltbwG/f3ocxEB0awGM3L+SFvFKmxYexMD3a2+EppZRXTfhEsKu8iQdW5nNWdjxLs2J5Ia+UW57eQI/D8MNLZmJtkKOUUj5rQicCu8PJD1/azqTQQH5/4wKiQgO4ITedr/4jj60ljXxmYYq3Q1RKKa+b0IngqXUH2FnWzGM3LyQq1LXaZ1RoAP/88hnUt3X3WzJCKaV8lVfmEYjIJSKyT0QKRORHnrpOQmQQ152ayqVzkvqV2/xEk4BSSllGvUYgIjbgUeBCoBTYKCIrjDG7T/a1Pr0glU8vSD3Zb6uUUhOKN2oEi4ACY0yRMaYbeA64ygtxKKWUwjuJIAUo6fO81CrrR0RuF5E8EcmrqakZteCUUsrXjNm1howxTxhjco0xufHxusKnUkp5ijcSQRnQdzpvqlWmlFLKC7yRCDYC2SIyVUQCgRuBFV6IQymlFF4YNWSMsYvIN4C3ARvwtDFm12jHoZRSysUrE8qMMW8Ab3jj2koppfobs53FSimlRof07tM7lolIDXDwOF8eB9SexHA8QWM8OTTGEzfW4wON8VhMMcYMO+xyXCSCEyEiecaYXG/HMRSN8eTQGE/cWI8PNEZP0KYhpZTycZoIlFLKx/lCInjC2wGMgMZ4cmiMJ26sxwca40k34fsIlFJKDc0XagRKKaWGMKETwWhtgHMM8aSJyBoR2S0iu0TkTqs8RkRWich+699JYyBWm4hsEZHXrOdTReQT614+by0P4s34okXkRRHZKyJ7ROTMsXYfReQ71v/zThF5VkSCvX0fReRpEakWkZ19yga8b+LyiBXrdhFZ6MUYf2v9X28XkZdFJLrPsbutGPeJyMXeirHPsbtExIhInPXcK/fxWEzYRNBnA5xLgdnATSIy27tRYQfuMsbMBs4A7rBi+hGw2hiTDay2nnvbncCePs9/DTxkjMkCGoDbvBLVYQ8DbxljZgLzcMU6Zu6jiKQA3wJyjTFzcC2nciPev49/BS45omyw+3YpkG193Q487sUYVwFzjDGnAPnA3QDW78+NQI71mses331vxIiIpAEXAYf6FHvrPo7YhE0EjMENcIwxFcaYzdbjFlx/vFKsuJZbpy0HrvZOhC4ikgpcDjxpPRfgPOBF6xSvxigiUcDZwFMAxphuY0wjY+w+4lrCJURE/IFQoAIv30djzPtA/RHFg923q4C/GZePgWgRmeyNGI0xK40xduvpx7hWLe6N8TljTJcx5gBQgOt3f9RjtDwE/ADo2/nqlft4LCZyIhjRBjjeIiIZwALgEyDRGFNhHaoEEr0UVq/f4fphdlrPY4HGPr+I3r6XU4Ea4C9W89WTIhLGGLqPxpgy4H5cnwwrgCZgE2PrPvYa7L6N1d+hLwFvWo/HTIwichVQZozZdsShMRPjYCZyIhizRCQceAn4tjGmue8x4xrG5bWhXCJyBVBtjNnkrRhGwB9YCDxujFkAtHFEM9AYuI+TcH0SnAokA2EM0JQw1nj7vg1HRO7B1cT6T2/H0peIhAI/Bv7b27Ecj4mcCMbkBjgiEoArCfzTGPNvq7iqt6po/VvtrfiAJcAyESnG1Zx2Hq72+GiriQO8fy9LgVJjzCfW8xdxJYaxdB8vAA4YY2qMMT3Av3Hd27F0H3sNdt/G1O+QiHwRuAK42Rwe9z5WYszElfS3Wb87qcBmEUli7MQ4qImcCMbcBjhWW/tTwB5jzIN9Dq0AbrEe3wK8Otqx9TLG3G2MSTXGZOC6Z+8aY24G1gDXWqd5O8ZKoEREZlhF5wO7GUP3EVeT0BkiEmr9v/fGOGbuYx+D3bcVwBesUS9nAE19mpBGlYhcgqu5cpkxpr3PoRXAjSISJCJTcXXIbhjt+IwxO4wxCcaYDOt3pxRYaP2sjpn7OChjzIT9Ai7DNcKgELhnDMSzFFe1ezuw1fq6DFcb/GpgP/AOEOPtWK14zwFesx5Pw/ULVgC8AAR5Obb5QJ51L18BJo21+wjcB+wFdgJ/B4K8fR+BZ3H1WfTg+mN122D3DRBcI+8KgR24RkB5K8YCXO3svb83f+xz/j1WjPuAS70V4xHHi4E4b97HY/nSmcVKKeXjJnLTkFJKqRHQRKCUUj5OE4FSSvk4TQRKKeXjNBEopZSP00SgJjQRcYjI1j5fQy5EJyJfE5EvnITrFveuPnmMr7tYRO6zVgR9c/hXKHXi/Ic/RalxrcMYM3+kJxtj/ujJYEbgLFyTzs4C1nk5FuUjtEagfJL1if03IrJDRDaISJZVfq+IfM96/C1x7R2xXUSes8piROQVq+xjETnFKo8VkZXi2n/gSVyTiHqv9TnrGltF5E8DLZMsIjeIyFZcS1f/DvgzcKuIeHU2vPINmgjURBdyRNPQDX2ONRlj5gJ/wPXH90g/AhYY1xr4X7PK7gO2WGU/Bv5mlf8MWGeMyQFeBtIBRGQWcAOwxKqZOICbj7yQMeZ5XKvR7rRi2mFde9mJfPNKjYQ2DamJbqimoWf7/PvQAMe3A/8UkVdwLWMBrmVCrgEwxrxr1QQice2P8Bmr/HURabDOPx84FdjoWnKIEAZfDG86UGQ9DjOuPSuU8jhNBMqXmUEe97oc1x/4K4F7RGTucVxDgOXGmLuHPEkkD4gD/EVkNzDZair6pjHmg+O4rlIjpk1Dypfd0Off9X0PiIgfkGaMWQP8EIgCwoEPsJp2ROQcoNa49pR4H/isVX4prkXwwLWY27UikmAdixGRKUcGYozJBV7HtYfBb3Atkjhfk4AaDVojUBNdiPXJutdbxpjeIaSTRGQ70AXcdMTrbMA/rG0xBXjEGNMoIvcCT1uva+fw8s33Ac+KyC7gI6w9a40xu0XkJ8BKK7n0AHcABweIdSGuzuKvAw8OcFwpj9DVR5VPsjYPyTXG1Ho7FqW8TZuGlFLKx2mNQCmlfJzWCJRSysdpIlBKKR+niUAppXycJgKllPJxmgiUUsrHaSJQSikf9/8D+HkpxYfmZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Lillicrap, T. Hunt, J. Pritzel, A. Heess, N. Erez, T. Tassa, Y. Silver, D. & Wierstra, D. (2016). Continuous Control with Reinforcement Learning, In Proceedings of ICLR. https://arxiv.org/abs/1509.02971\n",
    "\n",
    "[2] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M.A. (2014). Deterministic Policy Gradient Algorithms. ICML. https://dl.acm.org/doi/10.5555/3044805.3044850\n",
    "\n",
    "[3] Watkins, C.J., Dayan, P. Technical Note: Q-Learning. Machine Learning 8, 279–292\n",
    "(1992). https://doi.org/10.1023/A:102267672231\n",
    "\n",
    "[4] Sutton R, Barto A, Reinforcement Learning: An Introduction, The MIT Press, 2018.\n",
    "\n",
    "[5] https://github.com/FlyienSHaDOw/deep-reinforcement-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
